{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection for Gesture Recognition\n",
    "\n",
    "This Python script utilizes the [Mediapipe](https://mediapipe.dev/) library and OpenCV to capture video from a web camera, perform real-time human pose estimation, and collect landmark data for specific actions. The purpose is to create a dataset for training a machine learning model to recognize these actions based on keypoint information.\n",
    "\n",
    "## Key Components:\n",
    "\n",
    "1. **Mediapipe Detection Functions:**\n",
    "   - Define a function `mediapipe_detection` to process an image using the Mediapipe Holistic model and obtain detection results for facial, pose, and hand landmarks.\n",
    "   - Define a function `draw_landmarks` to visually annotate an image with landmarks and connections for face, pose, and both left and right hands.\n",
    "\n",
    "2. **Data Extraction Functions:**\n",
    "   - Define a function `get_keypoints` to extract the 3D coordinates of left and right hand landmarks, pose landmarks, and face landmarks from a single image. The coordinates are flattened into an array for model input.\n",
    "\n",
    "3. **Data Folder Creation:**\n",
    "   - Define a function `make_data_folders` to create folders for training data. Each folder corresponds to a specific action and sequence/example within that action.\n",
    "\n",
    "4. **Data Collection:**\n",
    "   - Define a function `collect_data` to capture video frames from the device camera, use the Mediapipe Holistic model for landmark detection, and save keypoints for each frame. The collected data is organized into folders based on the action, sequence, and frame number.\n",
    "\n",
    "5. **Adding More Actions:**\n",
    "   - To add more actions, update the `ACTIONS` array with the names of additional actions you want to detect.\n",
    "\n",
    "6. **Main Execution:**\n",
    "   - Initialize the device camera and set up the Mediapipe model.\n",
    "   - Iterate through predefined actions, sequences, and frames to collect and save landmark data.\n",
    "   - Visualize the process by displaying the annotated video feed with landmarks.\n",
    "\n",
    "The automates the process of creating a labeled dataset for training a machine learning model to recognize specific human actions based on pose information. The keypoints extracted from the landmark data serve as input features for the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp \n",
    "import cv2 \n",
    "import seaborn \n",
    "import matplotlib.pyplot as plt \n",
    "import time \n",
    "import os \n",
    "import numpy as np\n",
    "from utils.config import  DATA_PATH, NUM_EXAMPLES, SEQUENCE_LENGTH, ACTIONS, mp_holistic, mp_drawing\n",
    "from utils.mp_helper import mediapipe_detection, draw_landmarks, get_keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_folders():\n",
    "    \"\"\"\n",
    "    Create folders for training data, one folder for each action and each sequence/example within that action.\n",
    "\n",
    "    Parameters:\n",
    "    - None\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    \n",
    "    # create folders for training data\n",
    "    for action in ACTIONS:\n",
    "        for sequences in range(NUM_EXAMPLES):\n",
    "            try:\n",
    "                # make folder for each example for each action \n",
    "                os.makedirs(os.path.join(DATA_PATH, action, str(sequences)))\n",
    "            except:\n",
    "                pass \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data():\n",
    "    \n",
    "    \"\"\"\n",
    "    Capture video from the device camera, use the Mediapipe Holistic model to collect landmark data for  \n",
    "    actions. Each action has a set amount of examples and each example has a set amount of frames. We save keypoints for each frame\n",
    "    and organize it in the folders accordingly. \n",
    "\n",
    "    Parameters:\n",
    "    - None\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    \n",
    "    make_data_folders()\n",
    "    \n",
    "    vc = cv2.VideoCapture(0) # open up device camera \n",
    "\n",
    "    # set mediapipe model \n",
    "    with mp_holistic.Holistic(min_detection_confidence =0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        \n",
    "        # go through each action \n",
    "        for action in ACTIONS:\n",
    "            \n",
    "            # create desired amount of examples\n",
    "            for seq in range(NUM_EXAMPLES):\n",
    "                \n",
    "                # collect data for each sequence length\n",
    "                for frame_num in range(SEQUENCE_LENGTH):\n",
    "                    \n",
    "                    # get frame from stream \n",
    "                    ret, frame = vc.read()\n",
    "                    \n",
    "                    # make detection\n",
    "                    image, results = mediapipe_detection(frame, holistic)\n",
    "                    \n",
    "                    # show detection\n",
    "                    draw_landmarks(image, results)\n",
    "                    \n",
    "                    # display useful info on collecting or starting new collection\n",
    "                    if frame_num == 0:\n",
    "                        cv2.putText(image, 'STARTING COLLECTION', (120, 200),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 4, cv2.LINE_AA)\n",
    "                        \n",
    "                        cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, seq), (15, 12),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                        cv2.waitKey(2000) # How long to wait before starting to collect frames \n",
    "                    else:\n",
    "                        cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, seq), (15, 12),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, .5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "                    # save kp \n",
    "                    keypoints = get_keypoints(results)\n",
    "                    np.save(os.path.join(DATA_PATH, action, str(seq), str(frame_num)), keypoints)\n",
    "                    \n",
    "                    # show the feed\n",
    "                    cv2.imshow('Live Feed', image)\n",
    "                    \n",
    "                    # break gracefully \n",
    "                    if cv2.waitKey(10) == ord('q'):\n",
    "                        break\n",
    "        vc.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gesture_Recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
